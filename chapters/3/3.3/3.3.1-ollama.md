# 3.3.1 Ollama + Anything/open-webui

Ollama是一个支持在本地运行大模型的工具，因其上手简单、支持各类扩展等优点，推出后迅速在开源社区广泛传播。

使用Ollama非常简单，分为三步：安装Ollama、下载模型、本地使用大模型对话。

第一步，安装Ollama：打开网址（网址：https://ollama.com/），点击下载按钮下载Ollama到本地。

第二步，下载模型：在终端打开ollama。启动后，使用Ollama run 模型名称，即可下载模型。

你可以根据任务要求以及电脑配置选择下载的模型，下图是社区下载次数最多的几个模型。

| 模型名称 | 可用参数规模 | 下载量 |
|----------|--------------|--------|
| llama3   | 8B, 70B      | 6.2M   |
| llama3.1 | 8B, 70B, 405B| 5.2M   |
| gemma    | 2B, 7B       | 4.1M   |
| qwen     | 0.5B, 1.8B, 4B, 32B, 72B, 110B | 4M |
| qwen2    | 0.5B, 1.5B, 7B, 72B | 3.5M |
| mistral  | 7B           | 3.4M   |
| phi3     | 3B, 14B      | 2.4M   |
| llama2   | 7B, 13B, 70B | 2.1M   |

*上图数据截止2024年9月20日*

注：*MAC电脑启动终端方法：点击屏幕右上角的搜索图标，输入"终端"，然后点击搜索结果中的终端应用即可打开。Windows电脑启动终端方法：按下Windows键+R键，在运行窗口中输入"cmd"，然后点击确定或按回车键即可打开命令提示符窗口。*

第三步，本地使用大模型对话：当模型下载完成后，直接使用`ollama run 模型名称`即可在终端与模型对话。如果在终端与模型对话不方便，推荐安装`Open WebUI`与大模型对话。